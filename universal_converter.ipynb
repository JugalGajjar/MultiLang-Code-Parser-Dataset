{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dbd2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading compiled grammars from grammars/languages.so...\n",
      "✓ Loaded: python\n",
      "✓ Loaded: c\n",
      "✓ Loaded: cpp\n",
      "✓ Loaded: java\n",
      "✓ Loaded: javascript\n",
      "✓ Loaded: typescript\n",
      "✓ Loaded: go\n",
      "✓ Loaded: ruby\n",
      "✓ Loaded: c_sharp\n",
      "✓ Loaded: scala\n",
      "\n",
      "Supported languages: ['python', 'c', 'cpp', 'java', 'javascript', 'typescript', 'go', 'ruby', 'c_sharp', 'scala']\n",
      "\n",
      "=== Testing ALL Language Parsers ===\n",
      "✓ python       parser works - root: module               children: 1\n",
      "✓ c            parser works - root: translation_unit     children: 1\n",
      "✓ cpp          parser works - root: translation_unit     children: 2\n",
      "✓ java         parser works - root: program              children: 1\n",
      "✓ javascript   parser works - root: program              children: 1\n",
      "✓ typescript   parser works - root: program              children: 1\n",
      "✓ go           parser works - root: source_file          children: 2\n",
      "✓ ruby         parser works - root: program              children: 1\n",
      "✓ scala        parser works - root: compilation_unit     children: 1\n",
      "\n",
      "Summary: 9 working, 0 failed\n",
      "Input: test_input\n",
      "Output: /Users/kamala/Documents/Github Projects/MultiLang-Code-Parser-Dataset/mlcpd_output_out\n",
      "Starting MLCPD Tree-Sitter Conversion...\n",
      "Found 10 parquet files:\n",
      "  - c_parsed_1.parquet\n",
      "  - c_sharp_parsed_2.parquet\n",
      "  - typescript_parsed_2.parquet\n",
      "  - cpp_parsed_1.parquet\n",
      "  - ruby_parsed_1.parquet\n",
      "  - java_parsed_1.parquet\n",
      "  - go_parsed_4.parquet\n",
      "  - scala_parsed_3.parquet\n",
      "  - python_parsed_1.parquet\n",
      "  - javascript_parsed_1.parquet\n",
      "\n",
      "==================================================\n",
      "Processing c_parsed_1.parquet...\n",
      "Reading test_input/c_parsed_1.parquet...\n",
      "✓ c: 105 nodes, 18 categorized\n",
      "✓ c: 482 nodes, 18 categorized\n",
      "✓ c: 196 nodes, 18 categorized\n",
      "✓ c: 55 nodes, 18 categorized\n",
      "✓ c: 1149 nodes, 18 categorized\n",
      "✓ c: 366 nodes, 18 categorized\n",
      "✓ c: 84 nodes, 18 categorized\n",
      "✓ c: 395 nodes, 18 categorized\n",
      "✓ c: 86 nodes, 18 categorized\n",
      "✓ c: 14 nodes, 18 categorized\n",
      "Saving 10 records to /Users/kamala/Documents/Github Projects/MultiLang-Code-Parser-Dataset/mlcpd_output_out/c_parsed_1.json...\n",
      "Completed: 10/10 successful\n",
      "\n",
      "==================================================\n",
      "Processing c_sharp_parsed_2.parquet...\n",
      "Reading test_input/c_sharp_parsed_2.parquet...\n",
      "✓ c_sharp: 37 nodes, 18 categorized\n",
      "✓ c_sharp: 63 nodes, 18 categorized\n",
      "✓ c_sharp: 113 nodes, 18 categorized\n",
      "✓ c_sharp: 185 nodes, 18 categorized\n",
      "✓ c_sharp: 115 nodes, 18 categorized\n",
      "✓ c_sharp: 46 nodes, 18 categorized\n",
      "✓ c_sharp: 108 nodes, 18 categorized\n",
      "✓ c_sharp: 138 nodes, 18 categorized\n",
      "✓ c_sharp: 101 nodes, 18 categorized\n",
      "✓ c_sharp: 20 nodes, 18 categorized\n",
      "Saving 10 records to /Users/kamala/Documents/Github Projects/MultiLang-Code-Parser-Dataset/mlcpd_output_out/c_sharp_parsed_2.json...\n",
      "Completed: 10/10 successful\n",
      "\n",
      "==================================================\n",
      "Processing typescript_parsed_2.parquet...\n",
      "Reading test_input/typescript_parsed_2.parquet...\n",
      "✓ typescript: 124 nodes, 18 categorized\n",
      "✓ typescript: 153 nodes, 18 categorized\n",
      "✓ typescript: 227 nodes, 18 categorized\n",
      "✓ typescript: 829 nodes, 18 categorized\n",
      "✓ typescript: 256 nodes, 18 categorized\n",
      "✓ typescript: 112 nodes, 18 categorized\n",
      "✓ typescript: 113 nodes, 18 categorized\n",
      "✓ typescript: 25 nodes, 18 categorized\n",
      "✓ typescript: 557 nodes, 18 categorized\n",
      "✓ typescript: 44 nodes, 18 categorized\n",
      "Saving 10 records to /Users/kamala/Documents/Github Projects/MultiLang-Code-Parser-Dataset/mlcpd_output_out/typescript_parsed_2.json...\n",
      "Completed: 10/10 successful\n",
      "\n",
      "==================================================\n",
      "Processing cpp_parsed_1.parquet...\n",
      "Reading test_input/cpp_parsed_1.parquet...\n",
      "✓ cpp: 1444 nodes, 18 categorized\n",
      "✓ cpp: 176 nodes, 18 categorized\n",
      "✓ cpp: 2049 nodes, 18 categorized\n",
      "✓ cpp: 336 nodes, 18 categorized\n",
      "✓ cpp: 27 nodes, 18 categorized\n",
      "✓ cpp: 371 nodes, 18 categorized\n",
      "✓ cpp: 82 nodes, 18 categorized\n",
      "✓ cpp: 1670 nodes, 18 categorized\n",
      "✓ cpp: 191 nodes, 18 categorized\n",
      "✓ cpp: 2347 nodes, 18 categorized\n",
      "Saving 10 records to /Users/kamala/Documents/Github Projects/MultiLang-Code-Parser-Dataset/mlcpd_output_out/cpp_parsed_1.json...\n",
      "Completed: 10/10 successful\n",
      "\n",
      "==================================================\n",
      "Processing ruby_parsed_1.parquet...\n",
      "Reading test_input/ruby_parsed_1.parquet...\n",
      "✓ ruby: 77 nodes, 18 categorized\n",
      "✓ ruby: 56 nodes, 18 categorized\n",
      "✓ ruby: 396 nodes, 18 categorized\n",
      "✓ ruby: 60 nodes, 18 categorized\n",
      "✓ ruby: 25 nodes, 18 categorized\n",
      "✓ ruby: 247 nodes, 18 categorized\n",
      "✓ ruby: 531 nodes, 18 categorized\n",
      "✓ ruby: 164 nodes, 18 categorized\n",
      "✓ ruby: 355 nodes, 18 categorized\n",
      "✓ ruby: 858 nodes, 18 categorized\n",
      "Saving 10 records to /Users/kamala/Documents/Github Projects/MultiLang-Code-Parser-Dataset/mlcpd_output_out/ruby_parsed_1.json...\n",
      "Completed: 10/10 successful\n",
      "\n",
      "==================================================\n",
      "Processing java_parsed_1.parquet...\n",
      "Reading test_input/java_parsed_1.parquet...\n",
      "✓ java: 48 nodes, 18 categorized\n",
      "✓ java: 59 nodes, 18 categorized\n",
      "✓ java: 47 nodes, 18 categorized\n",
      "✓ java: 276 nodes, 18 categorized\n",
      "✓ java: 738 nodes, 18 categorized\n",
      "✓ java: 63 nodes, 18 categorized\n",
      "✓ java: 336 nodes, 18 categorized\n",
      "✓ java: 841 nodes, 18 categorized\n",
      "✓ java: 21 nodes, 18 categorized\n",
      "✓ java: 73 nodes, 18 categorized\n",
      "Saving 10 records to /Users/kamala/Documents/Github Projects/MultiLang-Code-Parser-Dataset/mlcpd_output_out/java_parsed_1.json...\n",
      "Completed: 10/10 successful\n",
      "\n",
      "==================================================\n",
      "Processing go_parsed_4.parquet...\n",
      "Reading test_input/go_parsed_4.parquet...\n",
      "✓ go: 537 nodes, 18 categorized\n",
      "✓ go: 756 nodes, 18 categorized\n",
      "✓ go: 34 nodes, 18 categorized\n",
      "✓ go: 46 nodes, 18 categorized\n",
      "✓ go: 508 nodes, 18 categorized\n",
      "✓ go: 423 nodes, 18 categorized\n",
      "✓ go: 24 nodes, 18 categorized\n",
      "✓ go: 554 nodes, 18 categorized\n",
      "✓ go: 326 nodes, 18 categorized\n",
      "✓ go: 484 nodes, 18 categorized\n",
      "Saving 10 records to /Users/kamala/Documents/Github Projects/MultiLang-Code-Parser-Dataset/mlcpd_output_out/go_parsed_4.json...\n",
      "Completed: 10/10 successful\n",
      "\n",
      "==================================================\n",
      "Processing scala_parsed_3.parquet...\n",
      "Reading test_input/scala_parsed_3.parquet...\n",
      "✓ scala: 263 nodes, 18 categorized\n",
      "✓ scala: 253 nodes, 18 categorized\n",
      "✓ scala: 208 nodes, 18 categorized\n",
      "✓ scala: 782 nodes, 18 categorized\n",
      "✓ scala: 470 nodes, 18 categorized\n",
      "✓ scala: 85 nodes, 18 categorized\n",
      "✓ scala: 125 nodes, 18 categorized\n",
      "✓ scala: 289 nodes, 18 categorized\n",
      "✓ scala: 131 nodes, 18 categorized\n",
      "✓ scala: 808 nodes, 18 categorized\n",
      "Saving 10 records to /Users/kamala/Documents/Github Projects/MultiLang-Code-Parser-Dataset/mlcpd_output_out/scala_parsed_3.json...\n",
      "Completed: 10/10 successful\n",
      "\n",
      "==================================================\n",
      "Processing python_parsed_1.parquet...\n",
      "Reading test_input/python_parsed_1.parquet...\n",
      "✓ python: 426 nodes, 18 categorized\n",
      "✓ python: 16 nodes, 18 categorized\n",
      "✓ python: 312 nodes, 18 categorized\n",
      "✓ python: 155 nodes, 18 categorized\n",
      "✓ python: 1182 nodes, 18 categorized\n",
      "✓ python: 231 nodes, 18 categorized\n",
      "✓ python: 468 nodes, 18 categorized\n",
      "✓ python: 146 nodes, 18 categorized\n",
      "✓ python: 449 nodes, 18 categorized\n",
      "✓ python: 1522 nodes, 18 categorized\n",
      "Saving 10 records to /Users/kamala/Documents/Github Projects/MultiLang-Code-Parser-Dataset/mlcpd_output_out/python_parsed_1.json...\n",
      "Completed: 10/10 successful\n",
      "\n",
      "==================================================\n",
      "Processing javascript_parsed_1.parquet...\n",
      "Reading test_input/javascript_parsed_1.parquet...\n",
      "✓ javascript: 856 nodes, 18 categorized\n",
      "✓ javascript: 92 nodes, 18 categorized\n",
      "✓ javascript: 104 nodes, 18 categorized\n",
      "✓ javascript: 267 nodes, 18 categorized\n",
      "✓ javascript: 117 nodes, 18 categorized\n",
      "✓ javascript: 638 nodes, 18 categorized\n",
      "✓ javascript: 648 nodes, 18 categorized\n",
      "✓ javascript: 487 nodes, 18 categorized\n",
      "✓ javascript: 25 nodes, 18 categorized\n",
      "✓ javascript: 31 nodes, 18 categorized\n",
      "Saving 10 records to /Users/kamala/Documents/Github Projects/MultiLang-Code-Parser-Dataset/mlcpd_output_out/javascript_parsed_1.json...\n",
      "Completed: 10/10 successful\n",
      "\n",
      "==================================================\n",
      "PROCESSING SUMMARY\n",
      "==================================================\n",
      "c_parsed_1.parquet: 10/10 (100.0%)\n",
      "c_sharp_parsed_2.parquet: 10/10 (100.0%)\n",
      "typescript_parsed_2.parquet: 10/10 (100.0%)\n",
      "cpp_parsed_1.parquet: 10/10 (100.0%)\n",
      "ruby_parsed_1.parquet: 10/10 (100.0%)\n",
      "java_parsed_1.parquet: 10/10 (100.0%)\n",
      "go_parsed_4.parquet: 10/10 (100.0%)\n",
      "scala_parsed_3.parquet: 10/10 (100.0%)\n",
      "python_parsed_1.parquet: 10/10 (100.0%)\n",
      "javascript_parsed_1.parquet: 10/10 (100.0%)\n",
      "==================================================\n",
      "TOTAL: 100/100 (100.0% success rate)\n",
      "\n",
      "Done! JSON files in: /Users/kamala/Documents/Github Projects/MultiLang-Code-Parser-Dataset/mlcpd_output_out\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any\n",
    "from tree_sitter import Parser, Node, Language\n",
    "\n",
    "# %%\n",
    "# Load your compiled grammars\n",
    "print(\"Loading compiled grammars from grammars/languages.so...\")\n",
    "LANGUAGE_OBJECTS = {}\n",
    "\n",
    "languages_to_load = ['python', 'c', 'cpp', 'java', 'javascript', 'typescript', 'go', 'ruby', 'c_sharp', 'scala']\n",
    "successful_loads = []\n",
    "\n",
    "for lang in languages_to_load:\n",
    "    try:\n",
    "        lang_obj = Language('grammars/languages.so', lang)\n",
    "        LANGUAGE_OBJECTS[lang] = lang_obj\n",
    "        successful_loads.append(lang)\n",
    "        print(f\"✓ Loaded: {lang}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load {lang}: {e}\")\n",
    "\n",
    "SUPPORTED_LANGUAGES = {lang: lang for lang in successful_loads}\n",
    "print(f\"\\nSupported languages: {list(SUPPORTED_LANGUAGES.keys())}\")\n",
    "\n",
    "# %%\n",
    "def get_parser(language: str) -> Parser:\n",
    "    \"\"\"Get tree-sitter parser for specific language\"\"\"\n",
    "    # Handle language name variations\n",
    "    language_mapping = {\n",
    "        'csharp': 'c_sharp',\n",
    "        'c-sharp': 'c_sharp',\n",
    "    }\n",
    "    normalized_lang = language_mapping.get(language, language)\n",
    "    \n",
    "    if normalized_lang not in SUPPORTED_LANGUAGES:\n",
    "        raise ValueError(f\"Unsupported language: {language} -> {normalized_lang}\")\n",
    "    \n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGE_OBJECTS[normalized_lang])\n",
    "    return parser\n",
    "\n",
    "# %%\n",
    "# Test ALL loaded parsers\n",
    "def test_all_parsers():\n",
    "    \"\"\"Test that ALL loaded language parsers work\"\"\"\n",
    "    print(\"\\n=== Testing ALL Language Parsers ===\")\n",
    "    test_cases = {\n",
    "        'python': \"print('hello world')\",\n",
    "        'c': \"int main() { return 0; }\",\n",
    "        'cpp': \"#include <iostream>\\nint main() { return 0; }\",\n",
    "        'java': \"class Test { public static void main(String[] args) {} }\",\n",
    "        'javascript': \"console.log('hello');\",\n",
    "        'typescript': \"const message: string = 'hello';\",\n",
    "        'go': \"package main\\nfunc main() { println('hello') }\",\n",
    "        'ruby': \"puts 'hello'\",\n",
    "        'csharp': \"using System;\\nclass Program { static void Main() { Console.WriteLine('hello'); } }\",\n",
    "        'scala': \"object Hello { def main(args: Array[String]) = println('hello') }\"\n",
    "    }\n",
    "    \n",
    "    working_parsers = []\n",
    "    failed_parsers = []\n",
    "    \n",
    "    for lang in SUPPORTED_LANGUAGES.keys():\n",
    "        if lang in test_cases:\n",
    "            try:\n",
    "                parser = get_parser(lang)\n",
    "                test_code = test_cases[lang]\n",
    "                tree = parser.parse(bytes(test_code, 'utf8'))\n",
    "                working_parsers.append(lang)\n",
    "                print(f\"✓ {lang:12} parser works - root: {tree.root_node.type:20} children: {len(tree.root_node.children)}\")\n",
    "            except Exception as e:\n",
    "                failed_parsers.append(lang)\n",
    "                print(f\"✗ {lang:12} parser failed: {e}\")\n",
    "    \n",
    "    print(f\"\\nSummary: {len(working_parsers)} working, {len(failed_parsers)} failed\")\n",
    "    return working_parsers, failed_parsers\n",
    "\n",
    "# Test all parsers\n",
    "working_parsers, failed_parsers = test_all_parsers()\n",
    "\n",
    "# %%\n",
    "def hash_string(text: str) -> str:\n",
    "    return hashlib.sha256(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "def calculate_line_offsets(text: str) -> List[int]:\n",
    "    offsets = [0]\n",
    "    for i, char in enumerate(text):\n",
    "        if char == '\\n':\n",
    "            offsets.append(i + 1)\n",
    "    return offsets\n",
    "\n",
    "# %%\n",
    "def extract_ast_structure(tree, source_code: str, language: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Convert tree-sitter tree to structured nodes array using actual AST traversal\n",
    "    with controlled granularity - filters out purely syntactic tokens\n",
    "    \"\"\"\n",
    "    nodes = []\n",
    "    node_id_counter = 0\n",
    "    \n",
    "    # Define tokens to skip (pure punctuation/delimiters)\n",
    "    SKIP_TOKENS = {\n",
    "        '(', ')', '{', '}', '[', ']', ',', ';', ':', \n",
    "        '.', '->', '::', '...', '|', '&',\n",
    "        # Common keyword-only nodes that are redundant\n",
    "        'def', 'class', 'function', 'var', 'let', 'const',\n",
    "        'public', 'private', 'protected', 'static',\n",
    "        'if', 'else', 'for', 'while', 'return',\n",
    "        'import', 'from', 'as', 'export', 'package',\n",
    "        'try', 'catch', 'finally', 'throw', 'raise',\n",
    "        'new', 'delete', 'sizeof', 'typeof',\n",
    "        'async', 'await', 'yield',\n",
    "        # Add language-specific delimiters\n",
    "        'then', 'do', 'end', 'begin'\n",
    "    }\n",
    "    \n",
    "    # Define node types that should be skipped entirely\n",
    "    SKIP_NODE_TYPES = {\n",
    "        # Pure punctuation/delimiter types\n",
    "        '(', ')', '{', '}', '[', ']', ',', ';', ':', '.',\n",
    "        # Standalone keyword types that don't add semantic value\n",
    "        'def', 'class', 'return', 'if', 'else', 'elif',\n",
    "        'for', 'while', 'import', 'from', 'as',\n",
    "        'public', 'private', 'protected', 'static',\n",
    "        'const', 'var', 'let', 'function',\n",
    "        'try', 'catch', 'finally', 'throw',\n",
    "        'async', 'await', 'new', 'delete',\n",
    "        # Common tree-sitter delimiter types\n",
    "        'comment',  # Keep this if you want comments, remove if not\n",
    "    }\n",
    "    \n",
    "    def should_skip_node(node: Node) -> bool:\n",
    "        \"\"\"Determine if a node should be skipped based on granularity rules\"\"\"\n",
    "        try:\n",
    "            node_text = source_code[node.start_byte:node.end_byte].strip()\n",
    "        except (IndexError, AttributeError):\n",
    "            return False  # Keep the node if we can't extract text safely\n",
    "        \n",
    "        node_type = node.type\n",
    "        \n",
    "        # Skip if it's a pure punctuation/delimiter token\n",
    "        if node_text in SKIP_TOKENS:\n",
    "            return True\n",
    "        \n",
    "        # Skip if the node type itself is just punctuation\n",
    "        if node_type in SKIP_NODE_TYPES:\n",
    "            return True\n",
    "        \n",
    "        # Keep nodes that have semantic meaning:\n",
    "        # - identifiers (variable/function/class names)\n",
    "        # - operators (mathematical, logical, comparison)\n",
    "        # - literals (strings, numbers, booleans)\n",
    "        # - compound structures (expressions, statements, declarations)\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def traverse_node(node: Node, parent_id: Optional[int]) -> Optional[int]:\n",
    "        nonlocal node_id_counter\n",
    "        \n",
    "        # Check if we should skip this node\n",
    "        if should_skip_node(node):\n",
    "            # Don't create a node, but still traverse children\n",
    "            # and connect them to the current parent\n",
    "            for child in node.children:\n",
    "                traverse_node(child, parent_id)\n",
    "            return None\n",
    "        \n",
    "        current_id = node_id_counter\n",
    "        node_id_counter += 1\n",
    "        \n",
    "        # Extract node text from source code\n",
    "        node_text = source_code[node.start_byte:node.end_byte]\n",
    "        \n",
    "        # Build node data\n",
    "        node_data = {\n",
    "            \"id\": current_id,\n",
    "            \"type\": node.type,\n",
    "            \"text\": node_text,\n",
    "            \"parent\": parent_id,\n",
    "            \"children\": [],\n",
    "            \"start_point\": {\"row\": node.start_point[0], \"column\": node.start_point[1]},\n",
    "            \"end_point\": {\"row\": node.end_point[0], \"column\": node.end_point[1]}\n",
    "        }\n",
    "        \n",
    "        nodes.append(node_data)\n",
    "        \n",
    "        # Recursively traverse children\n",
    "        child_ids = []\n",
    "        for child in node.children:\n",
    "            child_id = traverse_node(child, current_id)\n",
    "            if child_id is not None:  # Only add non-skipped children\n",
    "                child_ids.append(child_id)\n",
    "        \n",
    "        nodes[current_id][\"children\"] = child_ids\n",
    "        return current_id\n",
    "    \n",
    "    # Start traversal from root\n",
    "    root_id = traverse_node(tree.root_node, None)\n",
    "    \n",
    "    return {\n",
    "        \"root\": tree.root_node.type,\n",
    "        \"nodes\": nodes\n",
    "    }\n",
    "\n",
    "# %%\n",
    "def categorize_nodes_by_keywords(ast_structure: Dict, language: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Categorize nodes using keyword matching in node types (no field_name needed)\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        \"declarations\": {\n",
    "            \"functions\": [],\n",
    "            \"variables\": [], \n",
    "            \"classes\": [],\n",
    "            \"imports\": [],\n",
    "            \"modules\": [],\n",
    "            \"enums\": []\n",
    "        },\n",
    "        \"statements\": {\n",
    "            \"expressions\": [],\n",
    "            \"assignments\": [],\n",
    "            \"loops\": [],\n",
    "            \"conditionals\": [],\n",
    "            \"returns\": [],\n",
    "            \"exceptions\": []\n",
    "        },\n",
    "        \"expressions\": {\n",
    "            \"calls\": [],\n",
    "            \"literals\": [],\n",
    "            \"identifiers\": [],\n",
    "            \"binary_operations\": [],\n",
    "            \"unary_operations\": [],\n",
    "            \"member_access\": []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for node in ast_structure['nodes']:\n",
    "        node_type = node['type'].lower()\n",
    "        \n",
    "        # Function declarations\n",
    "        if any(keyword in node_type for keyword in ['function', 'method', 'def ', 'func ']):\n",
    "            categories['declarations']['functions'].append(node['id'])\n",
    "        \n",
    "        # Class declarations\n",
    "        elif any(keyword in node_type for keyword in ['class', 'struct', 'interface', 'trait', 'union']):\n",
    "            categories['declarations']['classes'].append(node['id'])\n",
    "        \n",
    "        # Import statements\n",
    "        elif any(keyword in node_type for keyword in ['import', 'include', 'using', 'require']):\n",
    "            categories['declarations']['imports'].append(node['id'])\n",
    "\n",
    "        # Module/namespace/package declarations\n",
    "        elif any(keyword in node_type for keyword in ['module', 'namespace', 'package', 'export']):\n",
    "            categories['declarations']['modules'].append(node['id'])\n",
    "\n",
    "        # Enum declarations\n",
    "        elif any(keyword in node_type for keyword in ['enum', 'enumeration']):\n",
    "            categories['declarations']['enums'].append(node['id'])\n",
    "        \n",
    "        # Variable declarations\n",
    "        elif any(keyword in node_type for keyword in ['declaration', 'definition', 'variable', 'var ']):\n",
    "            categories['declarations']['variables'].append(node['id'])\n",
    "        \n",
    "        # Return statements\n",
    "        elif 'return' in node_type:\n",
    "            categories['statements']['returns'].append(node['id'])\n",
    "        \n",
    "        # Conditional statements\n",
    "        elif any(keyword in node_type for keyword in ['if', 'switch', 'case', 'conditional']):\n",
    "            categories['statements']['conditionals'].append(node['id'])\n",
    "        \n",
    "        # Loop statements\n",
    "        elif any(keyword in node_type for keyword in ['for', 'while', 'loop']):\n",
    "            categories['statements']['loops'].append(node['id'])\n",
    "\n",
    "        # Exception handling statements\n",
    "        elif any(keyword in node_type for keyword in ['try', 'except', 'catch', 'finally', 'throw', 'raise']):\n",
    "            categories['statements']['exceptions'].append(node['id'])\n",
    "        \n",
    "        # Assignment statements\n",
    "        elif any(keyword in node_type for keyword in ['assignment', 'assign']):\n",
    "            categories['statements']['assignments'].append(node['id'])\n",
    "        \n",
    "        # Expression statements\n",
    "        elif 'expression' in node_type:\n",
    "            categories['statements']['expressions'].append(node['id'])\n",
    "        \n",
    "        # Call expressions\n",
    "        elif any(keyword in node_type for keyword in ['call', 'invocation']):\n",
    "            categories['expressions']['calls'].append(node['id'])\n",
    "        \n",
    "        # Binary operations\n",
    "        elif any(keyword in node_type for keyword in ['binary', 'arithmetic', 'operator']):\n",
    "            categories['expressions']['binary_operations'].append(node['id'])\n",
    "        \n",
    "        # Unary operations  \n",
    "        elif 'unary' in node_type:\n",
    "            categories['expressions']['unary_operations'].append(node['id'])\n",
    "\n",
    "        # Member/field/property access\n",
    "        elif any(keyword in node_type for keyword in ['member', 'field', 'property', 'access', 'dot']):\n",
    "            categories['expressions']['member_access'].append(node['id'])\n",
    "        \n",
    "        # Identifiers\n",
    "        elif any(keyword in node_type for keyword in ['identifier', 'name', 'symbol']):\n",
    "            categories['expressions']['identifiers'].append(node['id'])\n",
    "        \n",
    "        # Literals\n",
    "        elif any(keyword in node_type for keyword in ['literal', 'string', 'number', 'integer', 'float', 'boolean']):\n",
    "            categories['expressions']['literals'].append(node['id'])\n",
    "    \n",
    "    return categories\n",
    "\n",
    "# (rest of your code remains identical)\n",
    "\n",
    "\n",
    "# %%\n",
    "def extract_name_from_text(text: str, language: str) -> str:\n",
    "    \"\"\"Extract name from node text using simple patterns\"\"\"\n",
    "    # Simple pattern matching for different languages\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"unknown\"\n",
    "    if language == 'python':\n",
    "        # Look for patterns like \"def function_name\" or \"class ClassName\"\n",
    "        if 'def ' in text:\n",
    "            return text.split('def ')[1].split('(')[0].strip()\n",
    "        elif 'class ' in text:\n",
    "            return text.split('class ')[1].split(':')[0].strip()\n",
    "    elif language in ['c', 'cpp', 'java', 'c_sharp']:\n",
    "        # Look for patterns like \"void functionName\" or \"class ClassName\"\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word in ['class', 'struct', 'interface', 'union'] and i + 1 < len(words):\n",
    "                return words[i + 1]\n",
    "            elif i > 0 and words[i-1] in ['void', 'int', 'string', 'bool']:\n",
    "                return word.split('(')[0]\n",
    "    elif language in ['javascript', 'typescript']:\n",
    "        # Look for patterns like \"function functionName\" or \"class ClassName\"\n",
    "        if 'function ' in text:\n",
    "            return text.split('function ')[1].split('(')[0].strip()\n",
    "        elif 'class ' in text:\n",
    "            return text.split('class ')[1].split('{')[0].split('extends')[0].strip()\n",
    "        # Arrow functions: const name = () => or let name = function\n",
    "        elif '=' in text and '=>' in text:\n",
    "            return text.split('=')[0].strip().split()[-1]\n",
    "        elif '=' in text and 'function' in text:\n",
    "            return text.split('=')[0].strip().split()[-1]\n",
    "    elif language == 'go':\n",
    "        # Look for patterns like \"func functionName\" or \"type StructName struct\"\n",
    "        if 'func ' in text:\n",
    "            func_part = text.split('func ')[1]\n",
    "            # Handle methods: func (receiver Type) methodName\n",
    "            if func_part.strip().startswith('('):\n",
    "                return func_part.split(')')[1].split('(')[0].strip()\n",
    "            else:\n",
    "                return func_part.split('(')[0].strip()\n",
    "        elif 'type ' in text and 'struct' in text:\n",
    "            return text.split('type ')[1].split('struct')[0].strip()\n",
    "        elif 'type ' in text and 'interface' in text:\n",
    "            return text.split('type ')[1].split('interface')[0].strip()\n",
    "    elif language == 'ruby':\n",
    "        # Look for patterns like \"def method_name\" or \"class ClassName\"\n",
    "        if 'def ' in text:\n",
    "            def_part = text.split('def ')[1]\n",
    "            # Handle method names (stop at parentheses, newline, or end)\n",
    "            for delimiter in ['(', '\\n', '\\r', ' ']:\n",
    "                if delimiter in def_part:\n",
    "                    return def_part.split(delimiter)[0].strip()\n",
    "            return def_part.strip()\n",
    "        elif 'class ' in text:\n",
    "            class_part = text.split('class ')[1]\n",
    "            # Stop at < (inheritance) or newline\n",
    "            for delimiter in ['<', '\\n', '\\r']:\n",
    "                if delimiter in class_part:\n",
    "                    return class_part.split(delimiter)[0].strip()\n",
    "            return class_part.strip()\n",
    "        elif 'module ' in text:\n",
    "            return text.split('module ')[1].split('\\n')[0].strip()\n",
    "    elif language == 'scala':\n",
    "        # Look for patterns like \"def methodName\" or \"class ClassName\" or \"object ObjectName\"\n",
    "        if 'def ' in text:\n",
    "            def_part = text.split('def ')[1]\n",
    "            # Handle generic types: def name[T]\n",
    "            if '[' in def_part:\n",
    "                return def_part.split('[')[0].strip()\n",
    "            elif '(' in def_part:\n",
    "                return def_part.split('(')[0].strip()\n",
    "            else:\n",
    "                return def_part.split(':')[0].split('=')[0].strip()\n",
    "        elif 'class ' in text:\n",
    "            class_part = text.split('class ')[1]\n",
    "            # Stop at [, (, or extends\n",
    "            for delimiter in ['[', '(', 'extends', '{']:\n",
    "                if delimiter in class_part:\n",
    "                    return class_part.split(delimiter)[0].strip()\n",
    "            return class_part.strip()\n",
    "        elif 'object ' in text:\n",
    "            object_part = text.split('object ')[1]\n",
    "            for delimiter in ['extends', '{', '\\n']:\n",
    "                if delimiter in object_part:\n",
    "                    return object_part.split(delimiter)[0].strip()\n",
    "            return object_part.strip()\n",
    "        elif 'trait ' in text:\n",
    "            return text.split('trait ')[1].split('[')[0].split('extends')[0].strip()\n",
    "    \n",
    "    return \"unknown\"\n",
    "\n",
    "# %%\n",
    "def create_simple_cross_language_map(ast_structure: Dict, categories: Dict, language: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Create cross-language mapping using node text analysis (no field_name needed)\n",
    "    \"\"\"\n",
    "    cross_map = {\n",
    "        \"function_declarations\": [],\n",
    "        \"class_declarations\": [], \n",
    "        \"import_statements\": []\n",
    "    }\n",
    "    \n",
    "    # Map function declarations by analyzing node text\n",
    "    for func_node_id in categories['declarations']['functions']:\n",
    "        try:\n",
    "            func_node = next((n for n in ast_structure['nodes'] if n['id'] == func_node_id), None)\n",
    "            if not func_node:\n",
    "                continue\n",
    "            \n",
    "            # Try to extract function name from text\n",
    "            func_text = func_node.get('text', '')\n",
    "            if not func_text:\n",
    "                continue\n",
    "                \n",
    "            func_name = extract_name_from_text(func_text, language)\n",
    "            \n",
    "            function_info = {\n",
    "                \"node_id\": func_node_id,\n",
    "                \"universal_type\": \"function\",\n",
    "                \"name\": func_name,\n",
    "                \"text_snippet\": func_text[:100]  # First 100 chars\n",
    "            }\n",
    "            cross_map[\"function_declarations\"].append(function_info)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to process function node {func_node_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Map class declarations\n",
    "    for class_node_id in categories['declarations']['classes']:\n",
    "        try:\n",
    "            class_node = next((n for n in ast_structure['nodes'] if n['id'] == class_node_id), None)\n",
    "            if not class_node:\n",
    "                continue\n",
    "            \n",
    "            class_text = class_node.get('text', '')\n",
    "            if not class_text:\n",
    "                continue\n",
    "                \n",
    "            class_name = extract_name_from_text(class_text, language)\n",
    "            \n",
    "            class_info = {\n",
    "                \"node_id\": class_node_id,\n",
    "                \"universal_type\": \"class\", \n",
    "                \"name\": class_name,\n",
    "                \"text_snippet\": class_text[:100]\n",
    "            }\n",
    "            cross_map[\"class_declarations\"].append(class_info)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to process class node {class_node_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Map import statements\n",
    "    for import_node_id in categories['declarations']['imports']:\n",
    "        try:\n",
    "            import_node = next((n for n in ast_structure['nodes'] if n['id'] == import_node_id), None)\n",
    "            if not import_node:\n",
    "                continue\n",
    "            \n",
    "            import_text = import_node.get('text', '')\n",
    "            if not import_text:\n",
    "                continue\n",
    "                \n",
    "            import_info = {\n",
    "                \"node_id\": import_node_id,\n",
    "                \"text\": import_text\n",
    "            }\n",
    "            cross_map[\"import_statements\"].append(import_info)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to process import node {import_node_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return cross_map\n",
    "# %%\n",
    "def convert_parquet_row_to_json(row: pd.Series) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Convert a single parquet row to our tree-sitter JSON schema\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle language name variations\n",
    "        language = row['language']\n",
    "        language_mapping = {\n",
    "            'c-sharp': 'c_sharp',\n",
    "            'csharp': 'c_sharp',\n",
    "        }\n",
    "        normalized_language = language_mapping.get(language, language)\n",
    "        \n",
    "        if normalized_language not in SUPPORTED_LANGUAGES:\n",
    "            print(f\"Skipping: {language} -> {normalized_language} not supported\")\n",
    "            return None\n",
    "            \n",
    "        # Parse the code\n",
    "        parser = get_parser(normalized_language)\n",
    "        source_bytes = bytes(row['code'], 'utf8')\n",
    "        tree = parser.parse(source_bytes)\n",
    "        \n",
    "        # Extract AST structure\n",
    "        ast_structure = extract_ast_structure(tree, row['code'], normalized_language)\n",
    "        \n",
    "        # Add categorization\n",
    "        node_categories = categorize_nodes_by_keywords(ast_structure, normalized_language)\n",
    "        cross_language_map = create_simple_cross_language_map(ast_structure, node_categories, normalized_language)\n",
    "        \n",
    "        # Build result\n",
    "        result = {\n",
    "            \"language\": normalized_language,\n",
    "            \"success\": True,\n",
    "            \"metadata\": {\n",
    "                \"lines\": int(row['line_count']),\n",
    "                \"avg_line_length\": float(row['avg_line_length']),\n",
    "                \"nodes\": len(ast_structure['nodes']),\n",
    "                \"errors\": 0,\n",
    "                \"source_hash\": hash_string(row['code']),\n",
    "                \"categorized_nodes\": sum(len(cats) for cats in node_categories.values())\n",
    "            },\n",
    "            \"ast\": ast_structure,\n",
    "            \"node_categories\": node_categories,\n",
    "            \"cross_language_map\": cross_language_map,\n",
    "            \"source_context\": {\n",
    "                \"original_source\": row['code'],\n",
    "                \"line_offsets\": calculate_line_offsets(row['code'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ {normalized_language}: {len(ast_structure['nodes'])} nodes, {result['metadata']['categorized_nodes']} categorized\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error converting {row.get('language', 'unknown')}: {str(e)[:100]}...\")\n",
    "        return None\n",
    "\n",
    "# %%\n",
    "def process_parquet_file(input_file: str, output_dir: str, max_rows: Optional[int] = None) -> Dict:\n",
    "    \"\"\"Process a single parquet file\"\"\"\n",
    "    print(f\"Reading {input_file}...\")\n",
    "    df = pd.read_parquet(input_file)\n",
    "    \n",
    "    if max_rows:\n",
    "        df = df.head(max_rows)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if idx % 10 == 0 and idx > 0:\n",
    "            print(f\"  Processed {idx}/{len(df)} rows...\")\n",
    "            \n",
    "        json_data = convert_parquet_row_to_json(row)\n",
    "        if json_data:\n",
    "            results.append(json_data)\n",
    "    \n",
    "    # Save results\n",
    "    input_filename = Path(input_file).stem\n",
    "    output_file = Path(output_dir) / f\"{input_filename}.json\"\n",
    "    \n",
    "    print(f\"Saving {len(results)} records to {output_file}...\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return {\n",
    "        \"input_file\": input_file,\n",
    "        \"total_rows\": len(df),\n",
    "        \"successful_conversions\": len(results)\n",
    "    }\n",
    "\n",
    "# %%\n",
    "def process_folder(input_folder: str, output_folder: str, max_rows_per_file: Optional[int] = None) -> None:\n",
    "    \"\"\"Main processing function\"\"\"\n",
    "    input_path = Path(input_folder)\n",
    "    output_path = Path(output_folder)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    parquet_files = list(input_path.glob(\"*.parquet\"))\n",
    "    print(f\"Found {len(parquet_files)} parquet files:\")\n",
    "    for file in parquet_files:\n",
    "        print(f\"  - {file.name}\")\n",
    "    \n",
    "    all_stats = []\n",
    "    for parquet_file in parquet_files:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing {parquet_file.name}...\")\n",
    "        stats = process_parquet_file(str(parquet_file), output_folder, max_rows_per_file)\n",
    "        all_stats.append(stats)\n",
    "        print(f\"Completed: {stats['successful_conversions']}/{stats['total_rows']} successful\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"PROCESSING SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    total_rows = sum(s['total_rows'] for s in all_stats)\n",
    "    total_success = sum(s['successful_conversions'] for s in all_stats)\n",
    "    \n",
    "    for stats in all_stats:\n",
    "        success_rate = stats['successful_conversions'] / stats['total_rows'] if stats['total_rows'] > 0 else 0\n",
    "        print(f\"{Path(stats['input_file']).name}: {stats['successful_conversions']}/{stats['total_rows']} ({success_rate:.1%})\")\n",
    "    \n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"TOTAL: {total_success}/{total_rows} ({total_success/total_rows:.1%} success rate)\")\n",
    "\n",
    "# %%\n",
    "# === CONFIGURATION ===\n",
    "INPUT_FOLDER = \"test_input\"  \n",
    "OUTPUT_FOLDER = Path.cwd() / \"mlcpd_output_out\"\n",
    "MAX_ROWS_PER_FILE = 10\n",
    "\n",
    "# Create output folder\n",
    "OUTPUT_FOLDER.mkdir(exist_ok=True)\n",
    "print(f\"Input: {INPUT_FOLDER}\")\n",
    "print(f\"Output: {OUTPUT_FOLDER}\")\n",
    "\n",
    "# %%\n",
    "# === RUN PROCESSING ===\n",
    "if __name__ == \"__main__\":\n",
    "    if not Path(INPUT_FOLDER).exists():\n",
    "        print(f\"ERROR: Input folder not found: {INPUT_FOLDER}\")\n",
    "        exit(1)\n",
    "        \n",
    "    print(\"Starting MLCPD Tree-Sitter Conversion...\")\n",
    "    process_folder(INPUT_FOLDER, OUTPUT_FOLDER, MAX_ROWS_PER_FILE)\n",
    "    print(f\"\\nDone! JSON files in: {OUTPUT_FOLDER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c0653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
